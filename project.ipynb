{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize MongoDB client\n",
    "See README.md for setup instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib.parse import quote_plus\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "username = quote_plus('common')\n",
    "password = quote_plus(os.environ.get('MONGODB_PASSWORD'))\n",
    "uri = f\"mongodb+srv://{username}:{password}@playervaluations.v7jevdf.mongodb.net/?retryWrites=true&w=majority\"\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import json\\n\\ndb = client[\\'player_valuations\\']\\ncollection = db[\\'players\\']\\nplayer = collection.find_one({\\'player_id\\': 10})\\n\\n# Print the result\\nif player:\\n    print(\"Player found:\", json.dumps(player, indent=4, default=str))\\nelse:\\n    print(\"No player found with player_id\", 65)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import json\n",
    "\n",
    "db = client['player_valuations']\n",
    "collection = db['players']\n",
    "player = collection.find_one({'player_id': 10})\n",
    "\n",
    "# Print the result\n",
    "if player:\n",
    "    print(\"Player found:\", json.dumps(player, indent=4, default=str))\n",
    "else:\n",
    "    print(\"No player found with player_id\", 65)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, MapType, StringType, IntegerType, DoubleType\n",
    "\n",
    "db = client['player_valuations']\n",
    "collection = db['players']\n",
    "res = collection.find()\n",
    "\n",
    "df = pd.DataFrame(list(res))\n",
    "df.drop(\"_id\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"last_season\", IntegerType(), True),\n",
    "    StructField(\"current_club_id\", IntegerType(), True),\n",
    "    StructField(\"player_code\", StringType(), True),\n",
    "    StructField(\"country_of_birth\", StringType(), True),\n",
    "    StructField(\"city_of_birth\", StringType(), True),\n",
    "    StructField(\"country_of_citizenship\", StringType(), True),\n",
    "    StructField(\"date_of_birth\", StringType(), True),\n",
    "    StructField(\"sub_position\", StringType(), True),\n",
    "    StructField(\"position\", StringType(), True),\n",
    "    StructField(\"foot\", StringType(), True),\n",
    "    StructField(\"height_in_cm\", DoubleType(), True),\n",
    "    StructField(\"contract_expiration_date\", StringType(), True),\n",
    "    StructField(\"agent_name\", StringType(), True),\n",
    "    StructField(\"image_url\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"current_club_domestic_competition_id\", StringType(), True),\n",
    "    StructField(\"current_club_name\", StringType(), True),\n",
    "    StructField(\"market_value_in_eur\", DoubleType(), True),\n",
    "    StructField(\"highest_market_value_in_eur\", DoubleType(), True),\n",
    "    StructField(\"valuations\", ArrayType(StructType([\n",
    "        StructField(\"player_id\", IntegerType(), True),\n",
    "        StructField(\"date\", StringType(), True),\n",
    "        StructField(\"datetime\", StringType(), True),\n",
    "        StructField(\"dateweek\", StringType(), True),\n",
    "        StructField(\"market_value_in_eur\", IntegerType(), True),\n",
    "        StructField(\"current_club_id\", IntegerType(), True),\n",
    "        StructField(\"player_club_domestic_competition_id\", StringType(), True),\n",
    "    ]), True), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting a spark session and extracting the raw data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "player_valuation_df=ss.createDataFrame(df, schema = schema)\n",
    "appearances_df = ss.read.csv(\"raw_data\\\\transfermarkt\\\\appearances.csv\", header=True, inferSchema=True)\n",
    "games_df = ss.read.option(\"multiline\",\"true\").json(\"raw_data\\\\transfermarkt\\\\games.json\")\n",
    "game_events_df = ss.read.csv(\"raw_data\\\\transfermarkt\\\\game_events.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some cleaning operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "player_valuation_df = player_valuation_df.drop(*[\"image_url\", \"url\", \"name\", \"player_code\"])\n",
    "player_valuation_df = player_valuation_df.withColumns({\n",
    "    \"current_club_id\": when(player_valuation_df[\"last_season\"] != 2023, -1).otherwise(player_valuation_df[\"current_club_id\"]),\n",
    "    \"current_club_domestic_competition_id\": when(player_valuation_df[\"last_season\"] != 2023, \"-1\").otherwise(player_valuation_df[\"current_club_domestic_competition_id\"]),\n",
    "    \"current_club_name\": when(player_valuation_df[\"last_season\"] != 2023, \"Retired\").otherwise(player_valuation_df[\"current_club_name\"]),\n",
    "    \"market_value_in_eur\": when(player_valuation_df[\"last_season\"] != 2023, 0).otherwise(player_valuation_df[\"market_value_in_eur\"])\n",
    "})\n",
    "\n",
    "games_df = games_df.drop(*[\"url\", \"aggregate\", \"home_club_formation\", \"away_club_formation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max market value player in 2023. If there are many with the same max value take them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+\n",
      "|first_name| last_name|market_value_in_eur|\n",
      "+----------+----------+-------------------+\n",
      "|    Kylian|    Mbappé|          180000000|\n",
      "|    Erling|   Haaland|          180000000|\n",
      "|      Jude|Bellingham|          180000000|\n",
      "+----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col, expr\n",
    "\n",
    "valuations = player_valuation_df.select(\"valuations\")\n",
    "flattened_valuations = valuations.select(explode(\"valuations\").alias(\"valuation\"))\n",
    "valuations2023 = flattened_valuations.filter(\"substring(valuation.date, 1, 4) = '2023'\")\n",
    "max_market_value_players_2023 = valuations2023\\\n",
    "    .select(\"valuation.*\").groupBy(\"player_id\").max(\"market_value_in_eur\")\\\n",
    "    .withColumnRenamed(\"max(market_value_in_eur)\", \"market_value_in_eur\")\\\n",
    "    .join(player_valuation_df.select(\"player_id\", \"first_name\", \"last_name\"), on=\"player_id\", how=\"inner\")\\\n",
    "    .orderBy('market_value_in_eur', ascending=False)\\\n",
    "    .select(\"first_name\", \"last_name\", \"market_value_in_eur\")\n",
    "\n",
    "max_value = max_market_value_players_2023.select(\"market_value_in_eur\").first()[\"market_value_in_eur\"]\n",
    "\n",
    "max_value_players = max_market_value_players_2023.filter(col(\"market_value_in_eur\") == max_value)\n",
    "\n",
    "max_value_players.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPL bookings exaample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+--------------------+----------+--------------------+------+\n",
      "|first_name|last_name|      home_club_name|      away_club_name|      date|         description|minute|\n",
      "+----------+---------+--------------------+--------------------+----------+--------------------+------+\n",
      "|    Gareth|    Barry|          Reading FC|Manchester City F...|2013-05-14|6. Yellow card  ,...|    37|\n",
      "|       NaN|  Ramires| Queens Park Rangers|Chelsea Football ...|2012-09-15|1. Yellow card  ,...|    14|\n",
      "|      Ryan| Bertrand| Queens Park Rangers|Chelsea Football ...|2012-09-15|1. Yellow card  ,...|    27|\n",
      "|      Loïc|     Rémy|Newcastle United ...|Aston Villa Footb...|2014-02-23|2. Yellow card  ,...|    90|\n",
      "|     Cheik|    Tioté|Newcastle United ...|Aston Villa Footb...|2014-02-23|6. Yellow card  ,...|    69|\n",
      "+----------+---------+--------------------+--------------------+----------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cards_Premier_League = games_df.join(game_events_df, on=\"game_id\", how=\"inner\")\\\n",
    "    .join(player_valuation_df, on=\"player_id\", how=\"inner\")\\\n",
    "    .filter((col(\"type\") == \"Cards\") & (col(\"competition_id\") == \"GB1\"))\\\n",
    "    .select(player_valuation_df.first_name, player_valuation_df.last_name, games_df.home_club_name, games_df.away_club_name, games_df.date, game_events_df.description, game_events_df.minute)  \n",
    "\n",
    "cards_Premier_League.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the bookings example into parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\stoya\\AppData\\Local\\Temp\\ipykernel_14288\\1385738950.py\", line 1, in <module>\n",
      "    cards_Premier_League.write.mode(\"overwrite\").parquet(\"raw_data\\\\transfermarkt\\\\epl_bookings\")\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 1721, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o280.parquet.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: Could not locate Hadoop executable: D:\\hadoop-3.3.6\\bin\\winutils.exe -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: Could not locate Hadoop executable: D:\\hadoop-3.3.6\\bin\\winutils.exe -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:618)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "                      ^^^^^^^^^^^^\n",
      "AttributeError: 'Source' object has no attribute 'asttext'\n"
     ]
    }
   ],
   "source": [
    "cards_Premier_League.write.mode(\"overwrite\").parquet(\"raw_data\\\\transfermarkt\\\\epl_bookings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing the MongoDB client and the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"client.close()\n",
    "ss.stop()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
