{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize MongoDB client\n",
    "See README.md for setup instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib.parse import quote_plus\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "username = quote_plus('common')\n",
    "password = quote_plus(os.environ.get('MONGODB_PASSWORD'))\n",
    "uri = f\"mongodb+srv://{username}:{password}@playervaluations.v7jevdf.mongodb.net/?retryWrites=true&w=majority\"\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import json\\n\\ndb = client[\\'player_valuations\\']\\ncollection = db[\\'players\\']\\nplayer = collection.find_one({\\'player_id\\': 10})\\n\\n# Print the result\\nif player:\\n    print(\"Player found:\", json.dumps(player, indent=4, default=str))\\nelse:\\n    print(\"No player found with player_id\", 65)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import json\n",
    "\n",
    "db = client['player_valuations']\n",
    "collection = db['players']\n",
    "player = collection.find_one({'player_id': 10})\n",
    "\n",
    "# Print the result\n",
    "if player:\n",
    "    print(\"Player found:\", json.dumps(player, indent=4, default=str))\n",
    "else:\n",
    "    print(\"No player found with player_id\", 65)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, MapType, StringType, IntegerType, DoubleType\n",
    "\n",
    "db = client['player_valuations']\n",
    "collection = db['players']\n",
    "res = collection.find()\n",
    "\n",
    "df = pd.DataFrame(list(res))\n",
    "df.drop(\"_id\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"last_season\", IntegerType(), True),\n",
    "    StructField(\"current_club_id\", IntegerType(), True),\n",
    "    StructField(\"player_code\", StringType(), True),\n",
    "    StructField(\"country_of_birth\", StringType(), True),\n",
    "    StructField(\"city_of_birth\", StringType(), True),\n",
    "    StructField(\"country_of_citizenship\", StringType(), True),\n",
    "    StructField(\"date_of_birth\", StringType(), True),\n",
    "    StructField(\"sub_position\", StringType(), True),\n",
    "    StructField(\"position\", StringType(), True),\n",
    "    StructField(\"foot\", StringType(), True),\n",
    "    StructField(\"height_in_cm\", DoubleType(), True),\n",
    "    StructField(\"contract_expiration_date\", StringType(), True),\n",
    "    StructField(\"agent_name\", StringType(), True),\n",
    "    StructField(\"image_url\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"current_club_domestic_competition_id\", StringType(), True),\n",
    "    StructField(\"current_club_name\", StringType(), True),\n",
    "    StructField(\"market_value_in_eur\", DoubleType(), True),\n",
    "    StructField(\"highest_market_value_in_eur\", DoubleType(), True),\n",
    "    StructField(\"valuations\", ArrayType(StructType([\n",
    "        StructField(\"player_id\", IntegerType(), True),\n",
    "        StructField(\"date\", StringType(), True),\n",
    "        StructField(\"datetime\", StringType(), True),\n",
    "        StructField(\"dateweek\", StringType(), True),\n",
    "        StructField(\"market_value_in_eur\", IntegerType(), True),\n",
    "        StructField(\"current_club_id\", IntegerType(), True),\n",
    "        StructField(\"player_club_domestic_competition_id\", StringType(), True),\n",
    "    ]), True), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting a spark session and extracting the raw data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "player_valuation_df=ss.createDataFrame(df, schema = schema)\n",
    "appearances_df = ss.read.csv(\"raw_data\\\\transfermarkt\\\\appearances.csv\", header=True, inferSchema=True)\n",
    "games_df = ss.read.option(\"multiline\",\"true\").json(\"raw_data\\\\transfermarkt\\\\games.json\")\n",
    "game_events_df = ss.read.csv(\"raw_data\\\\transfermarkt\\\\game_events.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some cleaning operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "player_valuation_df = player_valuation_df.drop(*[\"image_url\", \"url\", \"name\", \"player_code\"])\n",
    "player_valuation_df = player_valuation_df.withColumns({\n",
    "    \"current_club_id\": when(player_valuation_df[\"last_season\"] != 2023, -1).otherwise(player_valuation_df[\"current_club_id\"]),\n",
    "    \"current_club_domestic_competition_id\": when(player_valuation_df[\"last_season\"] != 2023, \"-1\").otherwise(player_valuation_df[\"current_club_domestic_competition_id\"]),\n",
    "    \"current_club_name\": when(player_valuation_df[\"last_season\"] != 2023, \"Retired\").otherwise(player_valuation_df[\"current_club_name\"]),\n",
    "    \"market_value_in_eur\": when(player_valuation_df[\"last_season\"] != 2023, 0).otherwise(player_valuation_df[\"market_value_in_eur\"])\n",
    "})\n",
    "\n",
    "games_df = games_df.drop(*[\"url\", \"aggregate\", \"home_club_formation\", \"away_club_formation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max market value player in 2023. If there are many with the same max value take them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+\n",
      "|first_name| last_name|market_value_in_eur|\n",
      "+----------+----------+-------------------+\n",
      "|    Kylian|    Mbappé|          180000000|\n",
      "|    Erling|   Haaland|          180000000|\n",
      "|      Jude|Bellingham|          180000000|\n",
      "+----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col, expr\n",
    "\n",
    "valuations = player_valuation_df.select(\"valuations\")\n",
    "flattened_valuations = valuations.select(explode(\"valuations\").alias(\"valuation\"))\n",
    "valuations2023 = flattened_valuations.filter(\"substring(valuation.date, 1, 4) = '2023'\")\n",
    "max_market_value_players_2023 = valuations2023\\\n",
    "    .select(\"valuation.*\").groupBy(\"player_id\").max(\"market_value_in_eur\")\\\n",
    "    .withColumnRenamed(\"max(market_value_in_eur)\", \"market_value_in_eur\")\\\n",
    "    .join(player_valuation_df.select(\"player_id\", \"first_name\", \"last_name\"), on=\"player_id\", how=\"inner\")\\\n",
    "    .orderBy('market_value_in_eur', ascending=False)\\\n",
    "    .select(\"first_name\", \"last_name\", \"market_value_in_eur\")\n",
    "\n",
    "max_value = max_market_value_players_2023.select(\"market_value_in_eur\").first()[\"market_value_in_eur\"]\n",
    "\n",
    "max_value_players = max_market_value_players_2023.filter(col(\"market_value_in_eur\") == max_value)\n",
    "\n",
    "max_value_players.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPL bookings exaample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+--------------------+----------+--------------------+------+\n",
      "|first_name| last_name|      home_club_name|      away_club_name|      date|         description|minute|\n",
      "+----------+----------+--------------------+--------------------+----------+--------------------+------+\n",
      "|      Juan|      Cala|Tottenham Hotspur...|        Cardiff City|2014-03-02|1. Yellow card  ,...|    90|\n",
      "|       NaN|Diogo Jota|Tottenham Hotspur...|Liverpool Footbal...|2023-09-30|Second yellow  , ...|    69|\n",
      "|       NaN|Diogo Jota|Tottenham Hotspur...|Liverpool Footbal...|2023-09-30| Yellow card  , Foul|    68|\n",
      "|      Juan|      Cala|        Cardiff City|Liverpool Footbal...|2014-03-22|2. Yellow card  ,...|    33|\n",
      "|      Juan|      Cala|      Sunderland AFC|        Cardiff City|2014-04-27|Red card  , Profe...|    45|\n",
      "+----------+----------+--------------------+--------------------+----------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cards_Premier_League = games_df.join(game_events_df, on=\"game_id\", how=\"inner\")\\\n",
    "    .join(player_valuation_df, on=\"player_id\", how=\"inner\")\\\n",
    "    .filter((col(\"type\") == \"Cards\") & (col(\"competition_id\") == \"GB1\"))\\\n",
    "    .select(player_valuation_df.first_name, player_valuation_df.last_name, games_df.home_club_name, games_df.away_club_name, games_df.date, game_events_df.description, game_events_df.minute)  \n",
    "\n",
    "cards_Premier_League.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------+-----------+---------------+------------------+-------------+----------------------+-------------+--------------+----------+-----+------------+------------------------+--------------------+------------------------------------+-----------------+-------------------+---------------------------+--------------------+\n",
      "|player_id|first_name|   last_name|last_season|current_club_id|  country_of_birth|city_of_birth|country_of_citizenship|date_of_birth|  sub_position|  position| foot|height_in_cm|contract_expiration_date|          agent_name|current_club_domestic_competition_id|current_club_name|market_value_in_eur|highest_market_value_in_eur|          valuations|\n",
      "+---------+----------+------------+-----------+---------------+------------------+-------------+----------------------+-------------+--------------+----------+-----+------------+------------------------+--------------------+------------------------------------+-----------------+-------------------+---------------------------+--------------------+\n",
      "|       10|  Miroslav|       Klose|       2015|             -1|            Poland|        Opole|               Germany|   1978-06-09|Centre-Forward|    Attack|right|       184.0|                     NaN|ASBW Sport Marketing|                                  -1|          Retired|                0.0|                      3.0E7|[{10, 2004-10-04,...|\n",
      "|       26|     Roman|Weidenfeller|       2017|             -1|           Germany|         Diez|               Germany|   1980-08-06|    Goalkeeper|Goalkeeper| left|       190.0|                     NaN|    Neubauer 13 GmbH|                                  -1|          Retired|                0.0|                  8000000.0|[{26, 2004-10-04,...|\n",
      "|       65|   Dimitar|    Berbatov|       2015|             -1|          Bulgaria|  Blagoevgrad|              Bulgaria|   1981-01-30|Centre-Forward|    Attack|  NaN|         NaN|                     NaN|     CSKA-AS-23 Ltd.|                                  -1|          Retired|                0.0|                     3.45E7|[{65, 2004-10-04,...|\n",
      "|       77|       NaN|       Lúcio|       2012|             -1|            Brazil|     Brasília|                Brazil|   1978-05-08|   Centre-Back|  Defender|  NaN|         NaN|                     NaN|                 NaN|                                  -1|          Retired|                0.0|                     2.45E7|[{77, 2004-10-04,...|\n",
      "|       80|       Tom|      Starke|       2017|             -1|East Germany (GDR)|      Freital|               Germany|   1981-03-18|    Goalkeeper|Goalkeeper|right|       194.0|                     NaN|                 IFM|                                  -1|          Retired|                0.0|                  3000000.0|[{80, 2004-10-04,...|\n",
      "+---------+----------+------------+-----------+---------------+------------------+-------------+----------------------+-------------+--------------+----------+-----+------------+------------------------+--------------------+------------------------------------+-----------------+-------------------+---------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "player_valuation_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goals scored by each player, that still plays since the data is being collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pvd`.`first_name` cannot be resolved. Did you mean one of the following? [`pvd`.`player_id`, `total_goals`].;\n'Project ['pvd.first_name, 'pvd.last_name, total_goals#1132L]\n+- Sort [total_goals#1132L DESC NULLS LAST], true\n   +- Aggregate [player_id#0], [player_id#0, sum(CASE WHEN (type#158 = Goals) THEN 1 ELSE 0 END) AS total_goals#1132L]\n      +- Filter ((type#158 = Goals) AND (last_season#4 = 2023))\n         +- Join Inner, (player_id#0 = player_id#160)\n            :- SubqueryAlias pvd\n            :  +- Project [player_id#0, first_name#1, last_name#2, last_season#4, CASE WHEN NOT (last_season#4 = 2023) THEN -1 ELSE current_club_id#5 END AS current_club_id#194, country_of_birth#7, city_of_birth#8, country_of_citizenship#9, date_of_birth#10, sub_position#11, position#12, foot#13, height_in_cm#14, contract_expiration_date#15, agent_name#16, CASE WHEN NOT (last_season#4 = 2023) THEN -1 ELSE current_club_domestic_competition_id#19 END AS current_club_domestic_competition_id#195, CASE WHEN NOT (last_season#4 = 2023) THEN Retired ELSE current_club_name#20 END AS current_club_name#196, CASE WHEN NOT (last_season#4 = 2023) THEN cast(0 as double) ELSE market_value_in_eur#21 END AS market_value_in_eur#197, highest_market_value_in_eur#22, valuations#23]\n            :     +- Project [player_id#0, first_name#1, last_name#2, last_season#4, current_club_id#5, country_of_birth#7, city_of_birth#8, country_of_citizenship#9, date_of_birth#10, sub_position#11, position#12, foot#13, height_in_cm#14, contract_expiration_date#15, agent_name#16, current_club_domestic_competition_id#19, current_club_name#20, market_value_in_eur#21, highest_market_value_in_eur#22, valuations#23]\n            :        +- LogicalRDD [player_id#0, first_name#1, last_name#2, name#3, last_season#4, current_club_id#5, player_code#6, country_of_birth#7, city_of_birth#8, country_of_citizenship#9, date_of_birth#10, sub_position#11, position#12, foot#13, height_in_cm#14, contract_expiration_date#15, agent_name#16, image_url#17, url#18, current_club_domestic_competition_id#19, current_club_name#20, market_value_in_eur#21, highest_market_value_in_eur#22, valuations#23], false\n            +- SubqueryAlias ged\n               +- Relation [game_event_id#154,date#155,game_id#156,minute#157,type#158,club_id#159,player_id#160,description#161,player_in_id#162,player_assist_id#163] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 14\u001b[0m\n\u001b[0;32m      4\u001b[0m player_valuation_df_alias \u001b[38;5;241m=\u001b[39m player_valuation_df\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpvd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m game_events_df_alias \u001b[38;5;241m=\u001b[39m game_events_df\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mged\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m player_goals \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      8\u001b[0m     player_valuation_df_alias\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mjoin(game_events_df_alias, on\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpvd.player_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mged.player_id\u001b[39m\u001b[38;5;124m\"\u001b[39m), how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter((F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mged.type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoals\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m&\u001b[39m (F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpvd.last_season\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2023\u001b[39m))\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpvd.player_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39magg(F\u001b[38;5;241m.\u001b[39msum(F\u001b[38;5;241m.\u001b[39mwhen(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mged.type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoals\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_goals\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_goals\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpvd.first_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpvd.last_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_goals\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m player_goals\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:3036\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   2991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   2992\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   2993\u001b[0m \n\u001b[0;32m   2994\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3034\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[0;32m   3035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3036\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jcols(\u001b[38;5;241m*\u001b[39mcols))\n\u001b[0;32m   3037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pvd`.`first_name` cannot be resolved. Did you mean one of the following? [`pvd`.`player_id`, `total_goals`].;\n'Project ['pvd.first_name, 'pvd.last_name, total_goals#1132L]\n+- Sort [total_goals#1132L DESC NULLS LAST], true\n   +- Aggregate [player_id#0], [player_id#0, sum(CASE WHEN (type#158 = Goals) THEN 1 ELSE 0 END) AS total_goals#1132L]\n      +- Filter ((type#158 = Goals) AND (last_season#4 = 2023))\n         +- Join Inner, (player_id#0 = player_id#160)\n            :- SubqueryAlias pvd\n            :  +- Project [player_id#0, first_name#1, last_name#2, last_season#4, CASE WHEN NOT (last_season#4 = 2023) THEN -1 ELSE current_club_id#5 END AS current_club_id#194, country_of_birth#7, city_of_birth#8, country_of_citizenship#9, date_of_birth#10, sub_position#11, position#12, foot#13, height_in_cm#14, contract_expiration_date#15, agent_name#16, CASE WHEN NOT (last_season#4 = 2023) THEN -1 ELSE current_club_domestic_competition_id#19 END AS current_club_domestic_competition_id#195, CASE WHEN NOT (last_season#4 = 2023) THEN Retired ELSE current_club_name#20 END AS current_club_name#196, CASE WHEN NOT (last_season#4 = 2023) THEN cast(0 as double) ELSE market_value_in_eur#21 END AS market_value_in_eur#197, highest_market_value_in_eur#22, valuations#23]\n            :     +- Project [player_id#0, first_name#1, last_name#2, last_season#4, current_club_id#5, country_of_birth#7, city_of_birth#8, country_of_citizenship#9, date_of_birth#10, sub_position#11, position#12, foot#13, height_in_cm#14, contract_expiration_date#15, agent_name#16, current_club_domestic_competition_id#19, current_club_name#20, market_value_in_eur#21, highest_market_value_in_eur#22, valuations#23]\n            :        +- LogicalRDD [player_id#0, first_name#1, last_name#2, name#3, last_season#4, current_club_id#5, player_code#6, country_of_birth#7, city_of_birth#8, country_of_citizenship#9, date_of_birth#10, sub_position#11, position#12, foot#13, height_in_cm#14, contract_expiration_date#15, agent_name#16, image_url#17, url#18, current_club_domestic_competition_id#19, current_club_name#20, market_value_in_eur#21, highest_market_value_in_eur#22, valuations#23], false\n            +- SubqueryAlias ged\n               +- Relation [game_event_id#154,date#155,game_id#156,minute#157,type#158,club_id#159,player_id#160,description#161,player_in_id#162,player_assist_id#163] csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Provide aliases to the DataFrames\n",
    "player_valuation_df_alias = player_valuation_df.alias(\"pvd\")\n",
    "game_events_df_alias = game_events_df.alias(\"ged\")\n",
    "\n",
    "player_goals = (\n",
    "    player_valuation_df_alias\n",
    "    .join(game_events_df_alias, on=F.col(\"pvd.player_id\") == F.col(\"ged.player_id\"), how=\"inner\")\n",
    "    .filter((F.col(\"ged.type\") == \"Goals\") & (F.col(\"pvd.last_season\") == 2023))\n",
    "    .groupBy(\"pvd.player_id\")\n",
    "    .agg(F.sum(F.when(F.col(\"ged.type\") == \"Goals\", 1).otherwise(0)).alias(\"total_goals\"))\n",
    "    .orderBy(\"total_goals\", ascending=False)\n",
    "    .select(\"pvd.first_name\", \"pvd.last_name\", \"total_goals\")\n",
    ")\n",
    "\n",
    "player_goals.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the bookings example into parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\stoya\\AppData\\Local\\Temp\\ipykernel_14288\\1385738950.py\", line 1, in <module>\n",
      "    cards_Premier_League.write.mode(\"overwrite\").parquet(\"raw_data\\\\transfermarkt\\\\epl_bookings\")\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 1721, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o280.parquet.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: Could not locate Hadoop executable: D:\\hadoop-3.3.6\\bin\\winutils.exe -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: Could not locate Hadoop executable: D:\\hadoop-3.3.6\\bin\\winutils.exe -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:618)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\stoya\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "                      ^^^^^^^^^^^^\n",
      "AttributeError: 'Source' object has no attribute 'asttext'\n"
     ]
    }
   ],
   "source": [
    "cards_Premier_League.write.mode(\"overwrite\").parquet(\"raw_data\\\\transfermarkt\\\\epl_bookings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing the MongoDB client and the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"client.close()\n",
    "ss.stop()\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
